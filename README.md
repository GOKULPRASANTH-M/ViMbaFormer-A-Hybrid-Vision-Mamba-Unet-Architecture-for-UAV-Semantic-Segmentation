# ViMbaFormer  
## A Hybrid Visionâ€“Mamba U-Net Architecture for UAV Semantic Segmentation

ViMbaFormer is a hybrid semantic segmentation framework designed for high-resolution UAV imagery. It integrates **Vision Transformers**, **Mamba State Space Models**, and a **U-Netâ€“style encoderâ€“decoder** to jointly model global context and fine spatial details with linear-time complexity.

The architecture is tailored for aerial scene understanding, urban analysis, and autonomous UAV applications.

---

## ğŸš€ Highlights

- Hybrid CNN + Transformer + Mamba design  
- Visual State Space Blocks (VSSB) for long-range dependency modeling  
- Hybrid Attention (Mamba + Window Attention + Convolution)  
- Adaptive feature fusion decoder with boundary refinement  
- Lightweight and efficient for large UAV images  
- Strong performance on UAVid, UDD6, and SDD benchmarks  

---

## ğŸ§  Architecture Overview

ViMbaFormer consists of:

### Encoder
- CNN backbone for hierarchical spatial feature extraction  
- Multiple **Visual State Space Blocks (VSSB)** for global receptive fields  

### Bottleneck
- Hybrid attention combining:
  - Mamba selective state spaces (global context)
  - Local window attention
  - Convolutional spatial refinement  

### Decoder
- U-Net style upsampling  
- Adaptive fusion with learnable weights  
- Boundary-aware attention modules  

### Segmentation Head
- Produces pixel-wise class predictions.

This design balances:

- Local texture (CNN)  
- Global dependencies (Mamba + attention)  
- Efficient computation (linear-complexity SSMs)
  
---

## ğŸ“Š Supported Datasets

- UAVid (8 classes)  
- UDD6 (6 classes)  
- SDD (22 classes)  

### Dataset Format

```text
dataset/
â”œâ”€â”€ images/
â””â”€â”€ masks/
```

Images are padded and split into **1024Ã—1024 patches**:

- UAVid â†’ 8 patches / image  
- UDD6 â†’ 6 patches / image  
- SDD â†’ 24 patches / image  

This preserves full spatial information while enabling efficient training.

---

## ğŸ›  Installation

```bash
git clone https://github.com/yourusername/ViMbaFormer.git
cd ViMbaFormer
pip install -r requirements.txt
```

## â–¶ï¸ Training

```bash
python train.py --config config.yaml
```

Default settings:
- Optimizer: AdamW
- LR: 6e-5 (cosine annealing)
- Patch size: 1024Ã—1024
- Batch size: 2 (train), 8 (val)
- Loss: Cross-Entropy + Dice + Auxiliary supervision

## ğŸ§ª Testing
```bash
python test.py --checkpoint checkpoints/vimbaformer.pth
```

## ğŸ“ˆ Results
### ğŸ“Š Benchmark Results

- **UAVid**  
  - mIoU: **69.5%**

- **UDD6**  
  - mIoU: **79.9%**

- **SDD**  
  - mIoU: **74.1%**  
  - F1-score: **83.0%**

ViMbaFormer consistently outperforms CNN, Transformer, and prior Mamba baselines, especially on:
Fine structures
Small objects
Urban boundaries

## âš™ï¸ Model Complexity
| Model        | mIoU (UAVid) | Params (M) | MACs (G) |
|-------------|-------------|-----------|----------|
| LAPNet-L    | 66.0        | 8.6       | 25.4     |
| UNetFormer | 67.8        | 17.7      | 62.1     |
| **ViMbaFormer** | **69.5** | **19.0** | **68.4** |


## ğŸ”¬ Key Contributions
Hybrid attention combining Mamba + window attention + convolution
Visual State Space Blocks for linear-complexity global modeling
Adaptive decoder with learnable multi-scale fusion
Strong generalization across multiple UAV datasets

## ğŸ“œ Citation
If you use this work, please cite:
```bash
@article{vimbaformer2026,
  title={ViMbaFormer: A Hybrid Visionâ€“Mamba U-Net Architecture for UAV Semantic Segmentation},
  author={Mahalingam, Gokulprasanth and Deepak Kumar, B.S. and Vishnupriya, G. and Vasamsetti, Srikanth},
  year={2026}
}
```
